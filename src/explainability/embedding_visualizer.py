"""
Embedding Visualizer for DeepFake Detection.

This module provides visualization tools for understanding the embedding space
generated by DINOv2 and OpenCLIP models.
"""

import numpy as np
from typing import Optional, List, Tuple, Union
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.linear_model import LogisticRegression
from sklearn.metrics.pairwise import cosine_similarity

try:
    import umap
    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False


class EmbeddingVisualizer:
    """
    Visualization tools for embedding space analysis.
    
    This class provides methods to visualize high-dimensional embeddings
    using dimensionality reduction techniques like PCA, t-SNE, and UMAP.
    
    Attributes:
        embeddings: The embedding vectors
        labels: Class labels (0=real, 1=fake)
        reducer: The fitted dimensionality reduction model
    """
    
    def __init__(
        self,
        embeddings: Optional[np.ndarray] = None,
        labels: Optional[np.ndarray] = None
    ):
        """
        Initialize the Embedding Visualizer.
        
        Args:
            embeddings: Array of shape (n_samples, n_features)
            labels: Array of labels (0=real, 1=fake)
        """
        self.embeddings = embeddings
        self.labels = labels
        self.reducer = None
        self.reduced_embeddings = None
        
    def fit_pca(
        self,
        n_components: int = 2,
        embeddings: Optional[np.ndarray] = None
    ) -> np.ndarray:
        """
        Reduce embeddings using PCA.
        
        Args:
            n_components: Number of components to keep
            embeddings: Optional embeddings to use instead of stored ones
            
        Returns:
            Reduced embeddings of shape (n_samples, n_components)
        """
        emb = embeddings if embeddings is not None else self.embeddings
        if emb is None:
            raise ValueError("No embeddings provided")
            
        self.reducer = PCA(n_components=n_components, random_state=42)
        self.reduced_embeddings = self.reducer.fit_transform(emb)
        
        return self.reduced_embeddings
    
    def fit_tsne(
        self,
        n_components: int = 2,
        perplexity: float = 30.0,
        embeddings: Optional[np.ndarray] = None,
        **kwargs
    ) -> np.ndarray:
        """
        Reduce embeddings using t-SNE.
        
        Args:
            n_components: Number of components (usually 2)
            perplexity: Perplexity parameter for t-SNE
            embeddings: Optional embeddings to use instead of stored ones
            **kwargs: Additional arguments for t-SNE
            
        Returns:
            Reduced embeddings of shape (n_samples, n_components)
        """
        emb = embeddings if embeddings is not None else self.embeddings
        if emb is None:
            raise ValueError("No embeddings provided")
            
        default_params = {
            'n_components': n_components,
            'perplexity': perplexity,
            'random_state': 42,
            'n_iter': 1000
        }
        default_params.update(kwargs)
        
        self.reducer = TSNE(**default_params)
        self.reduced_embeddings = self.reducer.fit_transform(emb)
        
        return self.reduced_embeddings
    
    def fit_umap(
        self,
        n_components: int = 2,
        n_neighbors: int = 15,
        min_dist: float = 0.1,
        embeddings: Optional[np.ndarray] = None,
        **kwargs
    ) -> np.ndarray:
        """
        Reduce embeddings using UMAP.
        
        Args:
            n_components: Number of components
            n_neighbors: Number of neighbors for UMAP
            min_dist: Minimum distance parameter
            embeddings: Optional embeddings to use instead of stored ones
            **kwargs: Additional arguments for UMAP
            
        Returns:
            Reduced embeddings of shape (n_samples, n_components)
        """
        if not UMAP_AVAILABLE:
            raise ImportError("UMAP not installed. Install with: pip install umap-learn")
            
        emb = embeddings if embeddings is not None else self.embeddings
        if emb is None:
            raise ValueError("No embeddings provided")
            
        default_params = {
            'n_components': n_components,
            'n_neighbors': n_neighbors,
            'min_dist': min_dist,
            'random_state': 42
        }
        default_params.update(kwargs)
        
        self.reducer = umap.UMAP(**default_params)
        self.reduced_embeddings = self.reducer.fit_transform(emb)
        
        return self.reduced_embeddings
    
    def plot_2d(
        self,
        labels: Optional[np.ndarray] = None,
        title: str = "Embedding Space Visualization",
        figsize: Tuple[int, int] = (10, 8),
        alpha: float = 0.6,
        point_size: int = 50
    ) -> plt.Figure:
        """
        Create a 2D scatter plot of the reduced embeddings.
        
        Args:
            labels: Optional labels to color points by
            title: Plot title
            figsize: Figure size
            alpha: Point transparency
            point_size: Size of scatter points
            
        Returns:
            matplotlib Figure object
        """
        if self.reduced_embeddings is None:
            raise ValueError("No reduced embeddings. Call fit_pca, fit_tsne, or fit_umap first.")
            
        labels = labels if labels is not None else self.labels
        
        fig, ax = plt.subplots(figsize=figsize)
        
        if labels is not None:
            # Plot with class colors
            unique_labels = np.unique(labels)
            colors = {'Real': '#3498db', 'Fake': '#e74c3c'}
            
            for label in unique_labels:
                mask = labels == label
                label_name = 'Fake' if label == 1 else 'Real'
                ax.scatter(
                    self.reduced_embeddings[mask, 0],
                    self.reduced_embeddings[mask, 1],
                    c=colors[label_name],
                    label=label_name,
                    alpha=alpha,
                    s=point_size,
                    edgecolors='white',
                    linewidth=0.5
                )
            ax.legend(title='Class', loc='upper right')
        else:
            ax.scatter(
                self.reduced_embeddings[:, 0],
                self.reduced_embeddings[:, 1],
                alpha=alpha,
                s=point_size,
                edgecolors='white',
                linewidth=0.5
            )
        
        ax.set_xlabel('Component 1')
        ax.set_ylabel('Component 2')
        ax.set_title(title)
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        return fig
    
    def plot_decision_boundary(
        self,
        classifier,
        resolution: int = 100,
        title: str = "Decision Boundary in Embedding Space",
        figsize: Tuple[int, int] = (12, 8),
        cmap: str = 'RdBu'
    ) -> plt.Figure:
        """
        Visualize the decision boundary of a classifier in the reduced space.
        
        Note: This trains a new classifier on the reduced embeddings to
        approximate the decision boundary.
        
        Args:
            classifier: The original classifier (for reference)
            resolution: Grid resolution for boundary visualization
            title: Plot title
            figsize: Figure size
            cmap: Colormap for the decision boundary
            
        Returns:
            matplotlib Figure object
        """
        if self.reduced_embeddings is None or self.labels is None:
            raise ValueError("Reduced embeddings and labels required")
        
        # Train a simple classifier on reduced embeddings
        reduced_clf = LogisticRegression(max_iter=1000, random_state=42)
        reduced_clf.fit(self.reduced_embeddings, self.labels)
        
        # Create meshgrid
        x_min, x_max = self.reduced_embeddings[:, 0].min() - 1, self.reduced_embeddings[:, 0].max() + 1
        y_min, y_max = self.reduced_embeddings[:, 1].min() - 1, self.reduced_embeddings[:, 1].max() + 1
        
        xx, yy = np.meshgrid(
            np.linspace(x_min, x_max, resolution),
            np.linspace(y_min, y_max, resolution)
        )
        
        # Get predictions for the grid
        grid_points = np.c_[xx.ravel(), yy.ravel()]
        Z = reduced_clf.predict_proba(grid_points)[:, 1].reshape(xx.shape)
        
        fig, ax = plt.subplots(figsize=figsize)
        
        # Plot decision boundary
        contour = ax.contourf(xx, yy, Z, levels=50, cmap=cmap, alpha=0.4)
        plt.colorbar(contour, ax=ax, label='Fake Probability')
        
        # Plot data points
        mask_real = self.labels == 0
        mask_fake = self.labels == 1
        
        ax.scatter(
            self.reduced_embeddings[mask_real, 0],
            self.reduced_embeddings[mask_real, 1],
            c='#3498db', label='Real', alpha=0.6, edgecolors='white', s=50
        )
        ax.scatter(
            self.reduced_embeddings[mask_fake, 0],
            self.reduced_embeddings[mask_fake, 1],
            c='#e74c3c', label='Fake', alpha=0.6, edgecolors='white', s=50
        )
        
        # Plot decision boundary line
        ax.contour(xx, yy, Z, levels=[0.5], colors='black', linestyles='--', linewidths=2)
        
        ax.set_xlabel('Component 1')
        ax.set_ylabel('Component 2')
        ax.set_title(title)
        ax.legend(loc='upper right')
        
        plt.tight_layout()
        return fig
    
    def plot_embedding_distances(
        self,
        n_samples: int = 500,
        figsize: Tuple[int, int] = (12, 5),
        random_state: int = 42
    ) -> plt.Figure:
        """
        Analyze and plot distances between real and fake embeddings.
        
        Args:
            n_samples: Number of samples to use for distance calculation
            figsize: Figure size
            random_state: Random seed for reproducibility
            
        Returns:
            matplotlib Figure object
        """
        if self.embeddings is None or self.labels is None:
            raise ValueError("Embeddings and labels required")
        
        np.random.seed(random_state)
        
        # Sample data for efficiency
        real_mask = self.labels == 0
        fake_mask = self.labels == 1
        
        real_emb = self.embeddings[real_mask]
        fake_emb = self.embeddings[fake_mask]
        
        n_real = min(n_samples, len(real_emb))
        n_fake = min(n_samples, len(fake_emb))
        
        real_sample = real_emb[np.random.choice(len(real_emb), n_real, replace=False)]
        fake_sample = fake_emb[np.random.choice(len(fake_emb), n_fake, replace=False)]
        
        # Calculate distances
        real_real_cos = cosine_similarity(real_sample).flatten()
        fake_fake_cos = cosine_similarity(fake_sample).flatten()
        real_fake_cos = cosine_similarity(real_sample, fake_sample).flatten()
        
        fig, axes = plt.subplots(1, 2, figsize=figsize)
        
        # Cosine similarity distribution
        axes[0].hist(real_real_cos, bins=50, alpha=0.5, label='Real-Real', color='#3498db', density=True)
        axes[0].hist(fake_fake_cos, bins=50, alpha=0.5, label='Fake-Fake', color='#e74c3c', density=True)
        axes[0].hist(real_fake_cos, bins=50, alpha=0.5, label='Real-Fake', color='#9b59b6', density=True)
        axes[0].set_xlabel('Cosine Similarity')
        axes[0].set_ylabel('Density')
        axes[0].set_title('Cosine Similarity Distribution')
        axes[0].legend()
        
        # Box plot comparison
        data = [real_real_cos, fake_fake_cos, real_fake_cos]
        labels = ['Real-Real', 'Fake-Fake', 'Real-Fake']
        bp = axes[1].boxplot(data, labels=labels, patch_artist=True)
        colors = ['#3498db', '#e74c3c', '#9b59b6']
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.5)
        axes[1].set_ylabel('Cosine Similarity')
        axes[1].set_title('Similarity Comparison')
        
        plt.suptitle('Embedding Space Analysis: Real vs Fake', fontsize=12)
        plt.tight_layout()
        return fig
    
    def plot_pca_variance(
        self,
        n_components: int = 50,
        figsize: Tuple[int, int] = (12, 5)
    ) -> plt.Figure:
        """
        Plot PCA explained variance to understand embedding dimensionality.
        
        Args:
            n_components: Number of components to analyze
            figsize: Figure size
            
        Returns:
            matplotlib Figure object
        """
        if self.embeddings is None:
            raise ValueError("Embeddings required")
            
        n_components = min(n_components, self.embeddings.shape[1], self.embeddings.shape[0])
        pca = PCA(n_components=n_components, random_state=42)
        pca.fit(self.embeddings)
        
        fig, axes = plt.subplots(1, 2, figsize=figsize)
        
        # Individual explained variance
        axes[0].bar(range(1, n_components + 1), pca.explained_variance_ratio_, color='#3498db', alpha=0.7)
        axes[0].set_xlabel('Principal Component')
        axes[0].set_ylabel('Explained Variance Ratio')
        axes[0].set_title('Individual Explained Variance')
        
        # Cumulative explained variance
        cumsum = np.cumsum(pca.explained_variance_ratio_)
        axes[1].plot(range(1, n_components + 1), cumsum, 'b-', linewidth=2)
        axes[1].axhline(y=0.9, color='r', linestyle='--', label='90% variance')
        axes[1].axhline(y=0.95, color='g', linestyle='--', label='95% variance')
        axes[1].set_xlabel('Number of Components')
        axes[1].set_ylabel('Cumulative Explained Variance')
        axes[1].set_title('Cumulative Explained Variance')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        return fig
    
    def compare_embeddings(
        self,
        other_embeddings: np.ndarray,
        other_labels: np.ndarray,
        method: str = 'tsne',
        titles: Tuple[str, str] = ('DINOv2 Embeddings', 'OpenCLIP Embeddings'),
        figsize: Tuple[int, int] = (16, 7)
    ) -> plt.Figure:
        """
        Compare two different embedding spaces side by side.
        
        Useful for comparing DINOv2 vs OpenCLIP embeddings.
        
        Args:
            other_embeddings: Second set of embeddings
            other_labels: Labels for second set
            method: Reduction method ('pca', 'tsne', or 'umap')
            titles: Titles for each subplot
            figsize: Figure size
            
        Returns:
            matplotlib Figure object
        """
        # Create reducers for both embedding sets
        if method == 'pca':
            reducer1 = PCA(n_components=2, random_state=42)
            reducer2 = PCA(n_components=2, random_state=42)
        elif method == 'tsne':
            reducer1 = TSNE(n_components=2, random_state=42, perplexity=30)
            reducer2 = TSNE(n_components=2, random_state=42, perplexity=30)
        elif method == 'umap':
            if not UMAP_AVAILABLE:
                raise ImportError("UMAP not installed")
            reducer1 = umap.UMAP(n_components=2, random_state=42)
            reducer2 = umap.UMAP(n_components=2, random_state=42)
        else:
            raise ValueError(f"Unknown method: {method}")
            
        reduced1 = reducer1.fit_transform(self.embeddings)
        reduced2 = reducer2.fit_transform(other_embeddings)
        
        fig, axes = plt.subplots(1, 2, figsize=figsize)
        
        colors = {'Real': '#3498db', 'Fake': '#e74c3c'}
        
        for ax, reduced, labels, title in [
            (axes[0], reduced1, self.labels, titles[0]),
            (axes[1], reduced2, other_labels, titles[1])
        ]:
            for label_val in [0, 1]:
                mask = labels == label_val
                label_name = 'Fake' if label_val == 1 else 'Real'
                ax.scatter(
                    reduced[mask, 0],
                    reduced[mask, 1],
                    c=colors[label_name],
                    label=label_name,
                    alpha=0.6,
                    s=30,
                    edgecolors='white',
                    linewidth=0.3
                )
            ax.set_xlabel('Component 1')
            ax.set_ylabel('Component 2')
            ax.set_title(title)
            ax.legend()
            ax.grid(True, alpha=0.3)
            
        plt.suptitle(f'Embedding Comparison ({method.upper()})', fontsize=14)
        plt.tight_layout()
        return fig
